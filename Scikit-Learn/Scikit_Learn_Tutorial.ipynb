{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn: Machine Learning in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Scikit-learn is one of the most popular and user-friendly machine learning libraries in Python. It provides simple and efficient tools for data analysis and modeling, built on NumPy, SciPy, and matplotlib. This tutorial will guide you through the fundamentals of scikit-learn and demonstrate how to implement various machine learning algorithms for classification, regression, clustering, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scikit-learn?\n",
    "\n",
    "Scikit-learn has become the go-to library for machine learning in Python for several reasons:\n",
    "\n",
    "- **Consistent API**: All algorithms follow a consistent interface, making it easy to switch between different models\n",
    "- **Comprehensive Documentation**: Extensive documentation with examples and tutorials\n",
    "- **Active Community**: Large and active community providing support and contributions\n",
    "- **Integration**: Seamless integration with the scientific Python stack (NumPy, SciPy, Pandas, Matplotlib)\n",
    "- **Production-Ready**: Robust implementation suitable for both research and production environments\n",
    "- **Extensive Algorithm Coverage**: Implements a wide range of machine learning algorithms\n",
    "\n",
    "Let's start by importing scikit-learn and checking its version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn and check version\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Import other necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scikit-learn Core Concepts\n",
    "\n",
    "Before diving into specific algorithms, let's understand the core concepts and components of scikit-learn:\n",
    "\n",
    "### Concept: Estimators\n",
    "\n",
    "In scikit-learn, all machine learning algorithms are implemented as Python classes called **estimators**. Estimators implement the following methods:\n",
    "\n",
    "- `fit(X, y)`: Fit the model to the training data\n",
    "- `predict(X)`: Make predictions on new data\n",
    "- `score(X, y)`: Evaluate the model's performance\n",
    "\n",
    "Some estimators also implement additional methods like:\n",
    "\n",
    "- `transform(X)`: Transform the data (e.g., dimensionality reduction)\n",
    "- `fit_transform(X)`: Fit to data, then transform it\n",
    "- `predict_proba(X)`: Predict class probabilities\n",
    "\n",
    "This consistent API makes it easy to use different algorithms with minimal code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Datasets\n",
    "\n",
    "Scikit-learn provides several built-in datasets for practicing machine learning. These datasets are useful for learning and testing algorithms without having to download external data.\n",
    "\n",
    "Let's explore some of the built-in datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "print(\"Iris dataset:\")\n",
    "print(f\"- Number of samples: {iris.data.shape[0]}\")\n",
    "print(f\"- Number of features: {iris.data.shape[1]}\")\n",
    "print(f\"- Number of classes: {len(np.unique(iris.target))}\")\n",
    "print(f\"- Feature names: {iris.feature_names}\")\n",
    "print(f\"- Target names: {iris.target_names}\")\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "print(\"\\nDigits dataset:\")\n",
    "print(f\"- Number of samples: {digits.data.shape[0]}\")\n",
    "print(f\"- Number of features: {digits.data.shape[1]}\")\n",
    "print(f\"- Number of classes: {len(np.unique(digits.target))}\")\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = datasets.load_boston()\n",
    "print(\"\\nBoston housing dataset:\")\n",
    "print(f\"- Number of samples: {boston.data.shape[0]}\")\n",
    "print(f\"- Number of features: {boston.data.shape[1]}\")\n",
    "print(f\"- Feature names: {boston.feature_names[:5]}... (and more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Data Preprocessing\n",
    "\n",
    "Before applying machine learning algorithms, it's often necessary to preprocess the data. Scikit-learn provides various tools for data preprocessing, including:\n",
    "\n",
    "- **Standardization**: Scale features to have mean=0 and variance=1\n",
    "- **Normalization**: Scale features to a specific range (e.g., [0,1])\n",
    "- **Encoding**: Convert categorical variables to numerical\n",
    "- **Imputation**: Handle missing values\n",
    "- **Feature Selection**: Select the most relevant features\n",
    "\n",
    "Let's see some examples of data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Original data:\")\n",
    "print(X)\n",
    "\n",
    "# Standardization (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"\\nStandardized data (mean=0, std=1):\")\n",
    "print(X_scaled)\n",
    "print(f\"Mean: {X_scaled.mean(axis=0)}\")\n",
    "print(f\"Standard deviation: {X_scaled.std(axis=0)}\")\n",
    "\n",
    "# Min-Max scaling (normalization to [0,1] range)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_normalized = min_max_scaler.fit_transform(X)\n",
    "print(\"\\nNormalized data (range [0,1]):\")\n",
    "print(X_normalized)\n",
    "print(f\"Min: {X_normalized.min(axis=0)}\")\n",
    "print(f\"Max: {X_normalized.max(axis=0)}\")\n",
    "\n",
    "# Handling missing values\n",
    "X_missing = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "print(\"\\nData with missing values:\")\n",
    "print(X_missing)\n",
    "\n",
    "# Impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X_missing)\n",
    "print(\"\\nData after imputation (using mean):\")\n",
    "print(X_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Train-Test Split\n",
    "\n",
    "To evaluate a machine learning model properly, we need to split our data into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance on unseen data.\n",
    "\n",
    "Scikit-learn provides the `train_test_split` function for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total dataset size: {X.shape[0]} samples\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in original dataset\n",
    "print(\"\\nClass distribution in original dataset:\")\n",
    "for i, target_name in enumerate(iris.target_names):\n",
    "    print(f\"- {target_name}: {np.sum(y == i)} samples\")\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "for i, target_name in enumerate(iris.target_names):\n",
    "    print(f\"- {target_name}: {np.sum(y_train == i)} samples\")\n",
    "\n",
    "# Check class distribution in testing set\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "for i, target_name in enumerate(iris.target_names):\n",
    "    print(f\"- {target_name}: {np.sum(y_test == i)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Cross-Validation\n",
    "\n",
    "Cross-validation is a technique for evaluating machine learning models by training several models on different subsets of the available data and evaluating them on the complementary subset. This helps to assess how the model will generalize to an independent dataset.\n",
    "\n",
    "Scikit-learn provides several cross-validation strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a model\n",
    "model = LogisticRegression(max_iter=200, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"5-fold cross-validation scores:\")\n",
    "print(cv_scores)\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.4f}\")\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_scores = cross_val_score(model, X, y, cv=kf)\n",
    "print(\"\\nK-fold cross-validation scores:\")\n",
    "print(kf_scores)\n",
    "print(f\"Mean accuracy: {kf_scores.mean():.4f}\")\n",
    "\n",
    "# Stratified K-fold cross-validation (preserves class distribution)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "skf_scores = cross_val_score(model, X, y, cv=skf)\n",
    "print(\"\\nStratified K-fold cross-validation scores:\")\n",
    "print(skf_scores)\n",
    "print(f\"Mean accuracy: {skf_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Algorithms\n",
    "\n",
    "Classification is a supervised learning task where the goal is to predict the category (class) of new observations based on training data. Scikit-learn provides many classification algorithms, including:\n",
    "\n",
    "- Logistic Regression\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Naive Bayes\n",
    "\n",
    "Let's implement some of these algorithms on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train different classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(kernel='rbf', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Decision Boundaries\n",
    "\n",
    "Let's visualize the decision boundaries of different classifiers on a 2D projection of the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use PCA to reduce the iris dataset to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "# Define a function to plot decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    h = 0.02  # step size in the mesh\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict class for each point in the mesh\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot the training points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdYlBu)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title(title)\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=iris.target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train classifiers on the 2D data and plot decision boundaries\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_2d, y)\n",
    "    plot_decision_boundary(clf, X_2d, y, f\"Decision Boundary - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Algorithms\n",
    "\n",
    "Regression is a supervised learning task where the goal is to predict continuous values. Scikit-learn provides several regression algorithms, including:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Support Vector Regression (SVR)\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "\n",
    "Let's implement some of these algorithms on the Boston Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train different regressors\n",
    "regressors = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"Support Vector Regression\": SVR(kernel='rbf'),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each regressor\n",
    "for name, reg in regressors.items():\n",
    "    # Train the regressor\n",
    "    reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = reg.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f\"{name} - Actual vs Predicted Values\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Feature Importance\n",
    "\n",
    "Some regression models, like Random Forest, provide feature importance scores that indicate how useful each feature was in the construction of the model. Let's visualize the feature importance for the Boston Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_reg.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [boston.feature_names[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importance for Boston Housing Dataset\")\n",
    "plt.bar(range(X.shape[1]), importances[indices])\n",
    "plt.xticks(range(X.shape[1]), names, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{i+1}. {names[i]} ({importances[indices[i]]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Algorithms\n",
    "\n",
    "Clustering is an unsupervised learning task where the goal is to group similar data points together. Scikit-learn provides several clustering algorithms, including:\n",
    "\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Hierarchical Clustering\n",
    "- Gaussian Mixture Models\n",
    "\n",
    "Let's implement some of these algorithms on a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate a synthetic dataset with 3 clusters\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Plot the original data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', edgecolors='k', s=50)\n",
    "plt.title('Original Data with 3 Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create and train different clustering algorithms\n",
    "clustering_algorithms = {\n",
    "    \"K-Means\": KMeans(n_clusters=3, random_state=42),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5),\n",
    "    \"Agglomerative Clustering\": AgglomerativeClustering(n_clusters=3),\n",
    "    \"Gaussian Mixture Model\": GaussianMixture(n_components=3, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each clustering algorithm\n",
    "for name, algorithm in clustering_algorithms.items():\n",
    "    # Fit the algorithm\n",
    "    algorithm.fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    if name == \"Gaussian Mixture Model\":\n",
    "        y_pred = algorithm.predict(X)\n",
    "    else:\n",
    "        y_pred = algorithm.labels_\n",
    "    \n",
    "    # Calculate silhouette score (if there are at least 2 clusters and not all points are in the same cluster)\n",
    "    n_clusters = len(np.unique(y_pred))\n",
    "    if n_clusters > 1 and n_clusters < len(X):\n",
    "        silhouette_avg = silhouette_score(X, y_pred)\n",
    "        print(f\"{name} - Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    else:\n",
    "        print(f\"{name} - Silhouette Score: N/A (need at least 2 clusters)\")\n",
    "    \n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', edgecolors='k', s=50)\n",
    "    plt.title(f'Clusters Identified by {name}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Determining the Optimal Number of Clusters\n",
    "\n",
    "For algorithms like K-Means, we need to specify the number of clusters in advance. The Elbow Method and Silhouette Analysis are common techniques for determining the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method for K-Means\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the results\n",
    "print(\"Elbow Method and Silhouette Analysis Results:\")\n",
    "for k, inert, silhouette in zip(k_range, inertia, silhouette_scores):\n",
    "    print(f\"k={k}: Inertia={inert:.2f}, Silhouette Score={silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction techniques are used to reduce the number of features in a dataset while preserving as much information as possible. Scikit-learn provides several dimensionality reduction algorithms, including:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Truncated Singular Value Decomposition (TruncatedSVD)\n",
    "\n",
    "Let's implement some of these algorithms on the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Apply TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_svd = svd.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolors='k', alpha=0.7)\n",
    "axes[0].set_title('PCA')\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolors='k', alpha=0.7)\n",
    "axes[1].set_title('t-SNE')\n",
    "axes[1].set_xlabel('First t-SNE Component')\n",
    "axes[1].set_ylabel('Second t-SNE Component')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# TruncatedSVD\n",
    "scatter3 = axes[2].scatter(X_svd[:, 0], X_svd[:, 1], c=y, cmap='viridis', edgecolors='k', alpha=0.7)\n",
    "axes[2].set_title('TruncatedSVD')\n",
    "axes[2].set_xlabel('First SVD Component')\n",
    "axes[2].set_ylabel('Second SVD Component')\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Digit')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Digit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For PCA, let's also look at the explained variance ratio\n",
    "pca_full = PCA().fit(X_scaled)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), 'o-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the explained variance ratio for the first few components\n",
    "print(\"Explained variance ratio for the first 10 components:\")\n",
    "for i, ratio in enumerate(pca_full.explained_variance_ratio_[:10]):\n",
    "    print(f\"Component {i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance with 10 components: {np.sum(pca_full.explained_variance_ratio_[:10]):.4f} ({np.sum(pca_full.explained_variance_ratio_[:10])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection and Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning algorithm. Scikit-learn provides several tools for hyperparameter tuning, including:\n",
    "\n",
    "- Grid Search\n",
    "- Randomized Search\n",
    "\n",
    "Let's use these techniques to optimize a Support Vector Machine (SVM) classifier on the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(\"\\nTest set performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Define the parameter distributions for Randomized Search\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),\n",
    "    'gamma': uniform(0.001, 1),\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search = RandomizedSearchCV(svm, param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"\\nRandomized Search Results:\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_random = random_search.best_estimator_\n",
    "y_pred_random = best_model_random.predict(X_test_scaled)\n",
    "print(\"\\nTest set performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_random):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipelines\n",
    "\n",
    "Scikit-learn pipelines allow you to chain multiple steps together, such as preprocessing, feature selection, and model training. This helps to prevent data leakage and makes your code more organized.\n",
    "\n",
    "Let's create a pipeline for a classification task on the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=10)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Pipeline accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support()\n",
    "selected_feature_names = [cancer.feature_names[i] for i in range(len(cancer.feature_names)) if selected_features[i]]\n",
    "print(\"\\nSelected features:\")\n",
    "for feature in selected_feature_names:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Perform hyperparameter tuning on the pipeline\n",
    "param_grid = {\n",
    "    'feature_selection__k': [5, 10, 15, 20],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best pipeline on the test set\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "print(\"\\nTest set performance of best pipeline:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Persistence\n",
    "\n",
    "Once you've trained a model, you might want to save it for later use without having to retrain it. Scikit-learn provides the `joblib` module for model persistence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "\n",
    "# Create a directory for saving models if it doesn't exist\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Save the best pipeline to a file\n",
    "joblib.dump(best_pipeline, 'models/best_pipeline.pkl')\n",
    "\n",
    "# Load the model from the file\n",
    "loaded_model = joblib.load('models/best_pipeline.pkl')\n",
    "\n",
    "# Verify that the loaded model works correctly\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model accuracy: {accuracy_loaded:.4f}\")\n",
    "\n",
    "# Verify that the predictions are the same\n",
    "print(f\"Are predictions identical? {np.array_equal(y_pred_best, y_pred_loaded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "Now that you've learned the fundamentals of scikit-learn, try solving these practice problems to test your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Wine Classification\n",
    "\n",
    "Use the wine dataset from scikit-learn to train a classifier that can predict the wine type based on its chemical properties. Compare the performance of at least three different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create classifiers\n",
    "classifiers = {\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Diabetes Regression\n",
    "\n",
    "Use the diabetes dataset from scikit-learn to train a regression model that can predict the disease progression based on patient data. Implement a pipeline that includes feature scaling and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline with scaling, feature selection, and regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_regression, k=5)),\n",
    "    ('regressor', Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "print(f\"\\nCross-validation RMSE: {cv_rmse.mean():.4f} ± {cv_rmse.std():.4f}\")\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = pipeline.named_steps['feature_selection'].get_support()\n",
    "selected_feature_names = [diabetes.feature_names[i] for i in range(len(diabetes.feature_names)) if selected_features[i]]\n",
    "print(\"\\nSelected features:\")\n",
    "for feature in selected_feature_names:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values for Diabetes Progression')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Clustering Countries\n",
    "\n",
    "Use the scikit-learn clustering algorithms to group countries based on socio-economic indicators. You can use the World Bank or UN datasets, or create a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a synthetic dataset of countries with socio-economic indicators\n",
    "np.random.seed(42)\n",
    "n_countries = 50\n",
    "\n",
    "# Generate synthetic data\n",
    "gdp_per_capita = np.random.exponential(scale=15000, size=n_countries)\n",
    "life_expectancy = 50 + 30 * np.random.beta(5, 2, size=n_countries)\n",
    "literacy_rate = 40 + 60 * np.random.beta(5, 2, size=n_countries)\n",
    "infant_mortality = np.random.exponential(scale=30, size=n_countries)\n",
    "internet_users = np.random.beta(2, 5, size=n_countries) * 100\n",
    "\n",
    "# Create a DataFrame\n",
    "countries = [f\"Country_{i+1}\" for i in range(n_countries)]\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'GDP_per_capita': gdp_per_capita,\n",
    "    'Life_expectancy': life_expectancy,\n",
    "    'Literacy_rate': literacy_rate,\n",
    "    'Infant_mortality': infant_mortality,\n",
    "    'Internet_users': internet_users\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Synthetic Country Dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Prepare the data for clustering\n",
    "X = data.drop('Country', axis=1).values\n",
    "country_names = data['Country'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot the Elbow Method and Silhouette Scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose the optimal number of clusters (let's say k=4 based on the plots)\n",
    "optimal_k = 4\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Apply Hierarchical clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "data['KMeans_Cluster'] = kmeans_labels\n",
    "data['Hierarchical_Cluster'] = hierarchical_labels\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the clusters in 2D\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# K-Means\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(optimal_k):\n",
    "    plt.scatter(X_pca[kmeans_labels == i, 0], X_pca[kmeans_labels == i, 1], label=f'Cluster {i+1}')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hierarchical\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(optimal_k):\n",
    "    plt.scatter(X_pca[hierarchical_labels == i, 0], X_pca[hierarchical_labels == i, 1], label=f'Cluster {i+1}')\n",
    "plt.title('Hierarchical Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the clusters\n",
    "print(\"\\nK-Means Cluster Analysis:\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = data[data['KMeans_Cluster'] == i].drop(['KMeans_Cluster', 'Hierarchical_Cluster'], axis=1)\n",
    "    print(f\"\\nCluster {i+1} ({len(cluster_data)} countries):\")\n",
    "    print(cluster_data.describe().loc[['mean', 'std']].round(2))\n",
    "    print(f\"Countries in this cluster: {', '.join(cluster_data['Country'].values[:5])}\" + \n",
    "          (\"...\" if len(cluster_data) > 5 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "To further enhance your scikit-learn skills, check out these resources:\n",
    "\n",
    "- [Scikit-learn Official Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Scikit-learn Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Scikit-learn Examples](https://scikit-learn.org/stable/auto_examples/index.html)\n",
    "- [Python Machine Learning (Book) by Sebastian Raschka](https://sebastianraschka.com/books.html)\n",
    "- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Book) by Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

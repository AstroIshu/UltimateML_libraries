    "# # Scikit-learn: Machine Learning in Python\n",
    "# \n",
    "# ## Introduction\n",
    "# \n",
    "# Scikit-learn is one of the most popular and user-friendly machine learning libraries in Python. It provides simple and efficient tools for data analysis and modeling, built on NumPy, SciPy, and matplotlib.\n",
    "# \n",
    "# Key features of scikit-learn include:\n",
    "# \n",
    "# - Simple and consistent API\n",
    "# - Comprehensive documentation\n",
    "# - Wide variety of algorithms for classification, regression, clustering, dimensionality reduction, etc.\n",
    "# - Built-in dataset generators and sample datasets\n",
    "# - Tools for model evaluation and selection\n",
    "# - Seamless integration with the Python scientific stack\n",
    "# \n",
    "# **Source:** [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html) and [Scikit-learn GitHub Repository](https://github.com/scikit-learn/scikit-learn)\n",
    "\n",
    "# ## 1. Data Preparation and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing, model_selection\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# ### 1.1 Loading and Exploring Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the Iris dataset as an example\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to pandas DataFrame for easier exploration\n",
    "iris_df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "iris_df['target'] = y\n",
    "iris_df['species'] = iris_df['target'].map({\n",
    "    0: 'setosa',\n",
    "    1: 'versicolor',\n",
    "    2: 'virginica'\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Iris Dataset (first 5 rows):\")\n",
    "print(iris_df.head())\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(iris_df.describe())\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot of sepal length vs. sepal width\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, species in enumerate(['setosa', 'versicolor', 'virginica']):\n",
    "    plt.scatter(\n",
    "        iris_df[iris_df['species'] == species]['sepal length (cm)'],\n",
    "        iris_df[iris_df['species'] == species]['sepal width (cm)'],\n",
    "        label=species\n",
    "    )\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Sepal Length vs. Sepal Width')\n",
    "plt.legend()\n",
    "\n",
    "# Scatter plot of petal length vs. petal width\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, species in enumerate(['setosa', 'versicolor', 'virginica']):\n",
    "    plt.scatter(\n",
    "        iris_df[iris_df['species'] == species]['petal length (cm)'],\n",
    "        iris_df[iris_df['species'] == species]['petal width (cm)'],\n",
    "        label=species\n",
    "    )\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('Petal Length vs. Petal Width')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 1.2 Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "# Standardization (mean=0, std=1)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare original and scaled data\n",
    "print(\"\\nOriginal data (first 3 samples):\")\n",
    "print(X_train[:3])\n",
    "print(\"\\nScaled data (first 3 samples):\")\n",
    "print(X_train_scaled[:3])\n",
    "\n",
    "# Visualization of scaling effect\n",
    "features = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Before scaling\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.boxplot(X_train, labels=features)\n",
    "plt.title('Before Scaling')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# After scaling\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.boxplot(X_train_scaled, labels=features)\n",
    "plt.title('After Standardization')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Other preprocessing techniques\n",
    "# MinMaxScaler (scales features to a given range, default is [0, 1])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "# RobustScaler (scales features using statistics that are robust to outliers)\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalizer (scales samples to have unit norm)\n",
    "normalizer = preprocessing.Normalizer()\n",
    "X_train_normalized = normalizer.fit_transform(X_train)\n",
    "\n",
    "# Compare different scaling methods\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.boxplot(X_train, labels=features)\n",
    "plt.title('Original Data')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# StandardScaler\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.boxplot(X_train_scaled, labels=features)\n",
    "plt.title('StandardScaler')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# MinMaxScaler\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.boxplot(X_train_minmax, labels=features)\n",
    "plt.title('MinMaxScaler')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# RobustScaler\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.boxplot(X_train_robust, labels=features)\n",
    "plt.title('RobustScaler')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 1.3 Handling Missing Values and Categorical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a sample dataset with missing values and categorical features\n",
    "data = {\n",
    "    'age': [25, 30, np.nan, 40, 35],\n",
    "    'income': [50000, np.nan, 70000, 65000, 80000],\n",
    "    'gender': ['male', 'female', 'male', 'female', np.nan],\n",
    "    'education': ['bachelor', 'master', np.nan, 'phd', 'bachelor']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample dataset with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Handle missing numerical values\n",
    "# 1. Simple imputation with mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For numerical features\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[['age', 'income']] = num_imputer.fit_transform(df[['age', 'income']])\n",
    "\n",
    "# For categorical features\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['gender', 'education']] = cat_imputer.fit_transform(df[['gender', 'education']])\n",
    "\n",
    "print(\"\\nDataset after imputation:\")\n",
    "print(df)\n",
    "\n",
    "# Encoding categorical variables\n",
    "# 1. One-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Apply one-hot encoding to categorical variables\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_features = encoder.fit_transform(df[['gender', 'education']])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "for i, category in enumerate(['gender', 'education']):\n",
    "    feature_names.extend([f\"{category}_{val}\" for val in encoder.categories_[i][1:]])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "\n",
    "# Concatenate with the numerical features\n",
    "final_df = pd.concat([df[['age', 'income']].reset_index(drop=True), \n",
    "                      encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"\\nDataset after one-hot encoding:\")\n",
    "print(final_df)\n",
    "\n",
    "# 2. Label encoding (alternative for ordinal categorical variables)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original\n",
    "df_label_encoded = df.copy()\n",
    "\n",
    "# Apply label encoding to categorical variables\n",
    "for column in ['gender', 'education']:\n",
    "    le = LabelEncoder()\n",
    "    df_label_encoded[column] = le.fit_transform(df_label_encoded[column])\n",
    "\n",
    "print(\"\\nDataset after label encoding:\")\n",
    "print(df_label_encoded)\n",
    "\n",
    "\n",
    "# ## 2. Supervised Learning: Classification\n",
    "\n",
    "# ### 2.1 K-Nearest Neighbors (KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea35346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"KNN Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Visualize decision boundaries\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# We'll use only two features for visualization\n",
    "X_train_2d = X_train_scaled[:, :2]  # Using first two features\n",
    "X_test_2d = X_test_scaled[:, :2]\n",
    "\n",
    "# Train a KNN classifier on the 2D data\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Plot decision regions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_regions(X_train_2d, y_train, clf=knn_2d, legend=2)\n",
    "plt.xlabel('Standardized Sepal Length')\n",
    "plt.ylabel('Standardized Sepal Width')\n",
    "plt.title('KNN Decision Boundaries')\n",
    "plt.show()\n",
    "\n",
    "# Effect of k value\n",
    "k_values = list(range(1, 31, 2))\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
    "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, train_scores, 'o-', label='Training Accuracy')\n",
    "plt.plot(k_values, test_scores, 'o-', label='Testing Accuracy')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN: Effect of k on Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 2.2 Support Vector Machines (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06177ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"SVM Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (SVM)')\n",
    "plt.show()\n",
    "\n",
    "# Visualize decision boundaries for SVM\n",
    "# Train an SVM classifier on the 2D data\n",
    "svm_2d = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Plot decision regions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_decision_regions(X_train_2d, y_train, clf=svm_2d, legend=2)\n",
    "plt.xlabel('Standardized Sepal Length')\n",
    "plt.ylabel('Standardized Sepal Width')\n",
    "plt.title('SVM Decision Boundaries (RBF Kernel)')\n",
    "plt.show()\n",
    "\n",
    "# Effect of C parameter (regularization)\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "train_scores_C = []\n",
    "test_scores_C = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm = SVC(kernel='rbf', C=C, gamma='scale', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    train_scores_C.append(svm.score(X_train_scaled, y_train))\n",
    "    test_scores_C.append(svm.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(C_values, train_scores_C, 'o-', label='Training Accuracy')\n",
    "plt.semilogx(C_values, test_scores_C, 'o-', label='Testing Accuracy')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM: Effect of C on Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Different kernel types\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "train_scores_kernel = []\n",
    "test_scores_kernel = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    train_scores_kernel.append(svm.score(X_train_scaled, y_train))\n",
    "    test_scores_kernel.append(svm.score(X_test_scaled, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(kernels, train_scores_kernel, width=0.4, label='Training Accuracy', align='edge')\n",
    "plt.bar(kernels, test_scores_kernel, width=-0.4, label='Testing Accuracy', align='edge')\n",
    "plt.xlabel('Kernel Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVM: Performance with Different Kernels')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 2.3 Decision Trees and Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f44067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)  # Using original features for better interpretability\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Decision Tree)')\n",
    "plt.show()\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=iris.target_names))\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "importances_rf = rf.feature_importances_\n",
    "indices_rf = np.argsort(importances_rf)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances_rf[indices_rf], align='center')\n",
    "plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices_rf])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.show()\n",
    "\n",
    "# Effect of number of trees in Random Forest\n",
    "n_estimators = [1, 5, 10, 20, 50, 100, 200]\n",
    "train_scores_rf = []\n",
    "test_scores_rf = []\n",
    "\n",
    "for n in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores_rf.append(rf.score(X_train, y_train))\n",
    "    test_scores_rf.append(rf.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(n_estimators, train_scores_rf, 'o-', label='Training Accuracy')\n",
    "plt.semilogx(n_estimators, test_scores_rf, 'o-', label='Testing Accuracy')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: Effect of Number of Trees on Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## 3. Supervised Learning: Regression\n",
    "\n",
    "# ### 3.1 Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9920f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "# Split the data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = model_selection.train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler_d = preprocessing.StandardScaler()\n",
    "X_train_d_scaled = scaler_d.fit_transform(X_train_d)\n",
    "X_test_d_scaled = scaler_d.transform(X_test_d)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_d_scaled, y_train_d)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr.predict(X_test_d_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test_d, y_pred_lr)\n",
    "r2 = r2_score(y_test_d, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "# Coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': diabetes.feature_names,\n",
    "    'Coefficient': lr.coef_\n",
    "})\n",
    "coefficients = coefficients.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Coefficients in Linear Regression')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Actual vs. Predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_d, y_pred_lr, alpha=0.7)\n",
    "plt.plot([y_test_d.min(), y_test_d.max()], [y_test_d.min(), y_test_d.max()], 'r--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Linear Regression)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test_d - y_pred_lr\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_lr, residuals, alpha=0.7)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 3.2 Regularized Regression Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66763e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "# Train a Ridge Regression model (L2 regularization)\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_d_scaled, y_train_d)\n",
    "y_pred_ridge = ridge.predict(X_test_d_scaled)\n",
    "\n",
    "# Train a Lasso Regression model (L1 regularization)\n",
    "lasso = Lasso(alpha=0.1, random_state=42)\n",
    "lasso.fit(X_train_d_scaled, y_train_d)\n",
    "y_pred_lasso = lasso.predict(X_test_d_scaled)\n",
    "\n",
    "# Train an ElasticNet model (L1 + L2 regularization)\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic.fit(X_train_d_scaled, y_train_d)\n",
    "y_pred_elastic = elastic.predict(X_test_d_scaled)\n",
    "\n",
    "# Evaluate the models\n",
    "models = {\n",
    "    'Linear Regression': (lr, y_pred_lr),\n",
    "    'Ridge Regression': (ridge, y_pred_ridge),\n",
    "    'Lasso Regression': (lasso, y_pred_lasso),\n",
    "    'ElasticNet': (elastic, y_pred_elastic)\n",
    "}\n",
    "\n",
    "for name, (model, y_pred) in models.items():\n",
    "    mse = mean_squared_error(y_test_d, y_pred)\n",
    "    r2 = r2_score(y_test_d, y_pred)\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"  Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(mse):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Compare coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': diabetes.feature_names,\n",
    "    'Linear': lr.coef_,\n",
    "    'Ridge': ridge.coef_,\n",
    "    'Lasso': lasso.coef_,\n",
    "    'ElasticNet': elastic.coef_\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "coef_df.set_index('Feature').plot(kind='bar')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of Coefficients across Different Regression Models')\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend(title='Model')\n",
    "plt.show()\n",
    "\n",
    "# Effect of alpha in Ridge Regression\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_scores_ridge = []\n",
    "test_scores_ridge = []\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X_train_d_scaled, y_train_d)\n",
    "    train_scores_ridge.append(ridge.score(X_train_d_scaled, y_train_d))\n",
    "    test_scores_ridge.append(ridge.score(X_test_d_scaled, y_test_d))\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# R² scores\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(alphas, train_scores_ridge, 'o-', label='Training R²')\n",
    "plt.semilogx(alphas, test_scores_ridge, 'o-', label='Testing R²')\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Ridge Regression: Effect of Alpha on R² Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Coefficients\n",
    "plt.subplot(2, 1, 2)\n",
    "coefs = np.array(coefs)\n",
    "for i, feature in enumerate(diabetes.feature_names):\n",
    "    plt.semilogx(alphas, coefs[:, i], 'o-', label=feature)\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Ridge Regression: Effect of Alpha on Coefficients')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## 4. Unsupervised Learning\n",
    "\n",
    "# ### 4.1 K-Means Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fa440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data for clustering\n",
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.title('Synthetic Data for Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train a K-Means model\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Elbow method to find optimal number of clusters\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_blobs)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means to the Iris dataset\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels_iris = kmeans_iris.fit_predict(X)\n",
    "\n",
    "# Compare with true labels\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(y, cluster_labels_iris)\n",
    "nmi = normalized_mutual_info_score(y, cluster_labels_iris)\n",
    "\n",
    "print(\"K-Means Clustering on Iris Dataset:\")\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information: {nmi:.4f}\")\n",
    "\n",
    "# Visualize the clusters using the first two features\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# True labels\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.title('True Classes')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True)\n",
    "\n",
    "# K-Means clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels_iris, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(kmeans_iris.cluster_centers_[:, 0], kmeans_iris.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('K-Means Clusters')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 4.2 Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to the Iris dataset\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio:\")\n",
    "print(explained_variance)\n",
    "print(f\"Total explained variance: {sum(explained_variance):.4f}\")\n",
    "\n",
    "# Visualize the transformed data\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, target_name in enumerate(iris.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=target_name)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Loading vectors\n",
    "loadings = pca.components_\n",
    "loading_matrix = pd.DataFrame(loadings, columns=iris.feature_names)\n",
    "print(\"\\nPCA Loading Matrix:\")\n",
    "print(loading_matrix)\n",
    "\n",
    "# Visualize the loadings\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# First principal component\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(iris.feature_names, loadings[0])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Loading')\n",
    "plt.title('First Principal Component')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# Second principal component\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(iris.feature_names, loadings[1])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Loading')\n",
    "plt.title('Second Principal Component')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scree plot (explained variance by component)\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X)\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, label='Individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', label='Cumulative explained variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## 5. Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "# ### 5.1 Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15993ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name} Cross-Validation Scores: {scores}\")\n",
    "    print(f\"{name} Mean CV Score: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "    print()\n",
    "\n",
    "# Visualize the cross-validation results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([cv_results[name] for name in models.keys()], labels=models.keys())\n",
    "plt.title('Cross-Validation Results')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results_stratified = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "    cv_results_stratified[name] = scores\n",
    "    print(f\"{name} Stratified CV Scores: {scores}\")\n",
    "    print(f\"{name} Mean Stratified CV Score: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# ### 5.2 Hyperparameter Tuning with Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "print(\"\\nTest set accuracy with best model: {:.4f}\".format(accuracy_score(y_test, y_pred_best)))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Filter results for the RBF kernel\n",
    "rbf_results = results[results['param_kernel'] == 'rbf']\n",
    "pivot_table = rbf_results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_gamma',\n",
    "    columns='param_C'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.3f')\n",
    "plt.title('Grid Search Results (RBF Kernel)')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Gamma')\n",
    "plt.show()\n",
    "\n",
    "# Compare train and test scores for different C values with RBF kernel\n",
    "plt.figure(figsize=(10, 6))\n",
    "for gamma in ['scale', 'auto', 0.1, 0.01]:\n",
    "    subset = rbf_results[rbf_results['param_gamma'] == gamma]\n",
    "    plt.plot(subset['param_C'], subset['mean_test_score'], 'o-', label=f'gamma={gamma}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.title('SVM Performance with Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 5.3 Learning Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c295003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Generate learning curves for different models\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Generate learning curves for different models\n",
    "for name, model in models.items():\n",
    "    plot_learning_curve(\n",
    "        model, f'Learning Curve for {name}', X_scaled, y, cv=5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ## 6. Additional Resources for Further Learning\n",
    "\n",
    "# ### 6.1 Official Documentation and Tutorials\n",
    "\n",
    "# - [Scikit-learn Official Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "# - [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "# - [Scikit-learn Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
    "# - [API Reference](https://scikit-learn.org/stable/modules/classes.html)\n",
    "\n",
    "# ### 6.2 Online Courses and Books\n",
    "\n",
    "# - [Scikit-learn Course on DataCamp](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn)\n",
    "# - [Machine Learning with scikit-learn on Coursera](https://www.coursera.org/learn/python-machine-learning)\n",
    "# - [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "# - [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)\n",
    "# - [Python Machine Learning by Sebastian Raschka](https://sebastianraschka.com/books.html)\n",
    "\n",
    "# ### 6.3 GitHub Repositories and Examples\n",
    "\n",
    "# - [Scikit-learn GitHub Repository](https://github.com/scikit-learn/scikit-learn)\n",
    "# - [Scikit-learn Examples](https://scikit-learn.org/stable/auto_examples/index.html)\n",
    "# - [Machine Learning Notebooks by Aurélien Géron](https://github.com/ageron/handson-ml2)\n",
    "# - [Data Science IPython Notebooks](https://github.com/donnemartin/data-science-ipython-notebooks)\n",
    "# - [Python Machine Learning by Sebastian Raschka (Code Repository)](https://github.com/rasbt/python-machine-learning-book-3rd-edition)\n",
    "\n",
    "# ### 6.4 Cheat Sheets and Quick References\n",
    "\n",
    "# - [Scikit-learn Cheat Sheet (DataCamp)](https://www.datacamp.com/cheat-sheet/scikit-learn-cheat-sheet-python-machine-learning)\n",
    "# - [Machine Learning Algorithm Cheat Sheet (Microsoft)](https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet)\n",
    "# - [Choosing the Right Estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "# ## 7. Test Your Knowledge\n",
    "\n",
    "# ### Exercise 1: Classification\n",
    "# \n",
    "# 1. Load the breast cancer dataset from scikit-learn\n",
    "# 2. Split the data into training and testing sets\n",
    "# 3. Train a classifier of your choice (e.g., SVM, Random Forest)\n",
    "# 4. Evaluate the model using appropriate metrics\n",
    "# 5. Visualize the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea818dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Your code here\n",
    "# Example solution:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
    "\n",
    "# 5. Visualize the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer.target_names, \n",
    "            yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(10), importances[indices][:10], align='center')\n",
    "plt.xticks(range(10), [cancer.feature_names[i] for i in indices][:10], rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Exercise 2: Regression\n",
    "# \n",
    "# 1. Generate synthetic regression data using make_regression\n",
    "# 2. Split the data into training and testing sets\n",
    "# 3. Train a regression model of your choice\n",
    "# 4. Evaluate the model using appropriate metrics\n",
    "# 5. Plot the actual vs. predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Your code here\n",
    "# Example solution:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train a Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "y_pred = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse):.2f}\")\n",
    "\n",
    "# 5. Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.plot(np.sort(X_test, axis=0), ridge.predict(np.sort(X_test, axis=0)), color='green', label='Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Ridge Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Exercise 3: Clustering\n",
    "# \n",
    "# 1. Generate synthetic clustering data using make_blobs\n",
    "# 2. Apply K-means clustering with different values of k\n",
    "# 3. Use the elbow method to determine the optimal number of clusters\n",
    "# 4. Visualize the clustering results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d646f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Your code here\n",
    "# Example solution:\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Generate synthetic clustering data\n",
    "X, y = make_blobs(n_samples=300, centers=5, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# 2. Apply K-means clustering with different values of k\n",
    "# 3. Use the elbow method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Visualize the clustering results\n",
    "# Based on the elbow method, k=5 seems optimal\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('K-Means Clustering Results (k=5)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare with true labels\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "ari = adjusted_rand_score(y, cluster_labels)\n",
    "nmi = normalized_mutual_info_score(y, cluster_labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information: {nmi:.4f}\")\n",
    "\n",
    "\n",
    "# ## 8. Conclusion\n",
    "# \n",
    "# In this section, we've covered the fundamentals of scikit-learn, one of the most important libraries for machine learning in Python. We've explored:\n",
    "# \n",
    "# - Data preprocessing techniques\n",
    "# - Supervised learning algorithms for classification and regression\n",
    "# - Unsupervised learning algorithms for clustering and dimensionality reduction\n",
    "# - Model evaluation and hyperparameter tuning\n",
    "# \n",
    "# Scikit-learn provides a consistent API that makes it easy to use different algorithms with minimal code changes. This consistency, combined with excellent documentation and integration with the Python scientific stack, makes scikit-learn an essential tool for machine learning engineers.\n",
    "# \n",
    "# As you continue your journey to become a \"cracked MLE,\" I encourage you to explore the additional resources provided and practice implementing different algorithms on various datasets. The exercises in this notebook are just the beginning - the real learning comes from applying these techniques to solve real-world problems.\n",
    "# \n",
    "# Remember that scikit-learn is designed for traditional machine learning tasks. For deep learning, you'll want to explore libraries like TensorFlow and PyTorch, which we'll cover in future sections."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

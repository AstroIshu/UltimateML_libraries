{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas: Data Manipulation and Analysis in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Pandas is a powerful Python library for data manipulation and analysis. It provides data structures and functions needed to efficiently work with structured data. The two primary data structures in pandas are:\n",
    "\n",
    "- **DataFrame**: A 2-dimensional labeled data structure with columns that can be of different types\n",
    "- **Series**: A 1-dimensional labeled array capable of holding any data type\n",
    "\n",
    "This tutorial will introduce you to the most essential Pandas concepts and functions that are widely used in machine learning and data science projects.\n",
    "\n",
    "**Source:** [Pandas Documentation](https://pandas.pydata.org/docs/) and [Hands-On Data Analysis with Pandas](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pandas?\n",
    "\n",
    "Pandas bridges the gap between NumPy arrays and more complex data structures. It excels at:\n",
    "\n",
    "- **Data cleaning and preparation**: Handling missing data, filtering, transforming\n",
    "- **Data exploration**: Quick statistics, visualization integration\n",
    "- **Data manipulation**: Merging, reshaping, pivoting, time series functionality\n",
    "- **Working with diverse data formats**: CSV, Excel, SQL databases, JSON, and more\n",
    "- **Labeled data**: Intuitive handling of labeled data through index-based operations\n",
    "\n",
    "These capabilities make Pandas an essential tool for data scientists and machine learning engineers who need to prepare and analyze data before feeding it into models.\n",
    "\n",
    "### Key Advantages of Pandas:\n",
    "\n",
    "1. **Integrated data manipulation**: Combines SQL-like operations with spreadsheet-like functionality\n",
    "2. **Flexible handling of missing data**: Built-in methods for detecting and handling missing values\n",
    "3. **Intelligent data alignment**: Automatic alignment of data by labels\n",
    "4. **Powerful grouping and aggregation**: SQL-like group by operations with custom aggregations\n",
    "5. **Seamless integration**: Works well with NumPy, Matplotlib, scikit-learn, and other Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display settings for better output formatting\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Check pandas version\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas Data Structures\n",
    "\n",
    "### Concept: Series\n",
    "\n",
    "A Series is a one-dimensional labeled array capable of holding any data type. The axis labels are collectively called the index. Think of it as a single column of a spreadsheet or database table, or a specialized dictionary where the keys are the index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series from a list\n",
    "s1 = pd.Series([10, 20, 30, 40])\n",
    "print(\"Series with default integer index:\")\n",
    "print(s1)\n",
    "print()\n",
    "\n",
    "# Creating a Series with custom index\n",
    "s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "print(\"Series with custom string index:\")\n",
    "print(s2)\n",
    "print()\n",
    "\n",
    "# Creating a Series from a dictionary\n",
    "data_dict = {'a': 100, 'b': 200, 'c': 300, 'd': 400}\n",
    "s3 = pd.Series(data_dict)\n",
    "print(\"Series from dictionary:\")\n",
    "print(s3)\n",
    "print()\n",
    "\n",
    "# Series attributes\n",
    "print(\"Series values:\")\n",
    "print(s3.values)  # NumPy array of values\n",
    "print(\"\\nSeries index:\")\n",
    "print(s3.index)   # Index object\n",
    "print()\n",
    "\n",
    "# Accessing elements\n",
    "print(\"Element at index 'b':\", s3['b'])  # 200\n",
    "print(\"Elements at indices 'a' and 'c':\")\n",
    "print(s3[['a', 'c']])  # Series with values at 'a' and 'c'\n",
    "print()\n",
    "\n",
    "# Series operations\n",
    "s4 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n",
    "print(\"Original series:\")\n",
    "print(s4)\n",
    "print(\"\\nMultiply by 2:\")\n",
    "print(s4 * 2)  # Element-wise multiplication\n",
    "print(\"\\nAdd 5:\")\n",
    "print(s4 + 5)  # Element-wise addition\n",
    "print()\n",
    "\n",
    "# Series with mixed data types\n",
    "s5 = pd.Series([10, 'hello', True, 3.14])\n",
    "print(\"Series with mixed data types:\")\n",
    "print(s5)\n",
    "print(\"Data type:\", s5.dtype)  # object (Python's way of representing mixed types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: DataFrame\n",
    "\n",
    "A DataFrame is a 2-dimensional labeled data structure with columns that can be of different types. It's similar to a spreadsheet, SQL table, or a dictionary of Series objects. It's the most commonly used Pandas object and is at the core of most data analysis workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a dictionary of lists\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "    'Age': [28, 34, 29, 42],\n",
    "    'City': ['New York', 'Paris', 'Berlin', 'London'],\n",
    "    'Salary': [65000, 70000, 62000, 85000]\n",
    "}\n",
    "df1 = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary of lists:\")\n",
    "print(df1)\n",
    "print()\n",
    "\n",
    "# Creating a DataFrame from a 2D NumPy array\n",
    "array = np.random.rand(3, 4)  # 3 rows, 4 columns of random values\n",
    "df2 = pd.DataFrame(array, columns=['A', 'B', 'C', 'D'])\n",
    "print(\"DataFrame from 2D NumPy array:\")\n",
    "print(df2)\n",
    "print()\n",
    "\n",
    "# Creating a DataFrame from a list of dictionaries\n",
    "data_list = [\n",
    "    {'Name': 'John', 'Age': 28, 'City': 'New York'},\n",
    "    {'Name': 'Anna', 'Age': 34, 'City': 'Paris'},\n",
    "    {'Name': 'Peter', 'Age': 29, 'City': 'Berlin'},\n",
    "    {'Name': 'Linda', 'Age': 42, 'City': 'London'}\n",
    "]\n",
    "df3 = pd.DataFrame(data_list)\n",
    "print(\"DataFrame from list of dictionaries:\")\n",
    "print(df3)\n",
    "print()\n",
    "\n",
    "# DataFrame attributes\n",
    "print(\"DataFrame shape:\", df1.shape)  # (4, 4) - 4 rows, 4 columns\n",
    "print(\"DataFrame columns:\")\n",
    "print(df1.columns)  # Index(['Name', 'Age', 'City', 'Salary'], dtype='object')\n",
    "print(\"DataFrame index:\")\n",
    "print(df1.index)  # RangeIndex(start=0, stop=4, step=1)\n",
    "print(\"DataFrame data types:\")\n",
    "print(df1.dtypes)  # Data types of each column\n",
    "print()\n",
    "\n",
    "# Basic DataFrame information\n",
    "print(\"DataFrame info:\")\n",
    "df1.info()  # Summary of DataFrame including memory usage\n",
    "print(\"\\nDataFrame description (statistics):\")\n",
    "print(df1.describe())  # Statistical summary of numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Index Objects\n",
    "\n",
    "Index objects are immutable arrays that hold axis labels for Series and DataFrame objects. They enable fast lookups and data alignment operations. Understanding index objects is crucial for effective data manipulation in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Index objects\n",
    "idx1 = pd.Index(['a', 'b', 'c', 'd'])\n",
    "print(\"Basic Index:\")\n",
    "print(idx1)\n",
    "print(\"Type:\", type(idx1))\n",
    "print()\n",
    "\n",
    "# RangeIndex (default for DataFrames)\n",
    "idx2 = pd.RangeIndex(start=0, stop=10, step=2)\n",
    "print(\"RangeIndex:\")\n",
    "print(idx2)\n",
    "print(\"Values:\", list(idx2))  # [0, 2, 4, 6, 8]\n",
    "print()\n",
    "\n",
    "# DatetimeIndex for time series data\n",
    "idx3 = pd.date_range(start='2023-01-01', periods=5, freq='D')\n",
    "print(\"DatetimeIndex:\")\n",
    "print(idx3)\n",
    "print()\n",
    "\n",
    "# MultiIndex (hierarchical indexing)\n",
    "arrays = [[\"A\", \"A\", \"B\", \"B\"], [\"one\", \"two\", \"one\", \"two\"]]\n",
    "idx4 = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n",
    "print(\"MultiIndex:\")\n",
    "print(idx4)\n",
    "print()\n",
    "\n",
    "# Index operations\n",
    "idx5 = pd.Index(['a', 'b', 'c', 'd', 'e'])\n",
    "idx6 = pd.Index(['d', 'e', 'f', 'g'])\n",
    "\n",
    "print(\"Index 1:\", idx5)\n",
    "print(\"Index 2:\", idx6)\n",
    "print(\"Union:\", idx5.union(idx6))  # Combine unique values\n",
    "print(\"Intersection:\", idx5.intersection(idx6))  # Common values\n",
    "print(\"Difference:\", idx5.difference(idx6))  # Values in idx5 but not in idx6\n",
    "print(\"Symmetric difference:\", idx5.symmetric_difference(idx6))  # Values in either but not both\n",
    "print()\n",
    "\n",
    "# Using custom index in DataFrame\n",
    "df4 = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [10, 20, 30, 40]\n",
    "}, index=['w', 'x', 'y', 'z'])\n",
    "print(\"DataFrame with custom index:\")\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Selection and Manipulation\n",
    "\n",
    "### Concept: Selecting Data\n",
    "\n",
    "Pandas provides multiple ways to select data from DataFrames and Series. Understanding these selection methods is essential for effective data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Name': ['John', 'Anna', 'Peter', 'Linda', 'Bob'],\n",
    "    'Age': [28, 34, 29, 42, 37],\n",
    "    'City': ['New York', 'Paris', 'Berlin', 'London', 'Tokyo'],\n",
    "    'Department': ['IT', 'HR', 'IT', 'Finance', 'Marketing'],\n",
    "    'Salary': [65000, 70000, 62000, 85000, 72000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Selecting columns\n",
    "print(\"Single column (Series):\")\n",
    "print(df['Name'])  # Returns a Series\n",
    "print(\"\\nMultiple columns (DataFrame):\")\n",
    "print(df[['Name', 'Age', 'Salary']])  # Returns a DataFrame\n",
    "print()\n",
    "\n",
    "# Selecting rows by position (iloc)\n",
    "print(\"First row:\")\n",
    "print(df.iloc[0])  # Returns a Series (first row)\n",
    "print(\"\\nRows 1 to 3:\")\n",
    "print(df.iloc[1:4])  # Returns a DataFrame (rows 1, 2, 3)\n",
    "print(\"\\nSpecific rows and columns by position:\")\n",
    "print(df.iloc[0:2, 1:4])  # Rows 0-1, columns 1-3\n",
    "print()\n",
    "\n",
    "# Selecting rows by label (loc)\n",
    "# First, set the 'Name' column as the index\n",
    "df_indexed = df.set_index('Name')\n",
    "print(\"DataFrame with 'Name' as index:\")\n",
    "print(df_indexed)\n",
    "print(\"\\nSelect row by label:\")\n",
    "print(df_indexed.loc['John'])  # Returns a Series (row with index 'John')\n",
    "print(\"\\nSelect multiple rows by label:\")\n",
    "print(df_indexed.loc[['Anna', 'Bob']])  # Returns a DataFrame\n",
    "print(\"\\nSelect rows and columns by label:\")\n",
    "print(df_indexed.loc[['John', 'Linda'], ['Age', 'Salary']])  # Specific rows and columns\n",
    "print()\n",
    "\n",
    "# Boolean indexing\n",
    "print(\"Rows where Age > 30:\")\n",
    "print(df[df['Age'] > 30])  # Returns rows where the condition is True\n",
    "print(\"\\nRows where Department is 'IT':\")\n",
    "print(df[df['Department'] == 'IT'])\n",
    "print(\"\\nRows where Age > 30 AND Salary > 75000:\")\n",
    "print(df[(df['Age'] > 30) & (df['Salary'] > 75000)])\n",
    "print(\"\\nRows where Department is 'IT' OR 'HR':\")\n",
    "print(df[df['Department'].isin(['IT', 'HR'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Data Manipulation\n",
    "\n",
    "Pandas provides powerful functions for manipulating data, including adding/removing columns, applying functions, and sorting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the same DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Adding a new column\n",
    "df['Bonus'] = [5000, 7000, 4000, 10000, 6000]\n",
    "print(\"DataFrame with new 'Bonus' column:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Adding a calculated column\n",
    "df['Total Compensation'] = df['Salary'] + df['Bonus']\n",
    "print(\"DataFrame with calculated 'Total Compensation' column:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Applying a function to a column\n",
    "df['Age_Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else ('Middle-aged' if x < 40 else 'Senior'))\n",
    "print(\"DataFrame with 'Age_Group' column created using apply():\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Modifying values\n",
    "df.loc[df['Department'] == 'IT', 'Bonus'] *= 1.1  # 10% bonus increase for IT department\n",
    "print(\"DataFrame after increasing bonus for IT department:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Renaming columns\n",
    "df = df.rename(columns={'Total Compensation': 'Total_Comp', 'Age_Group': 'Generation'})\n",
    "print(\"DataFrame after renaming columns:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Dropping columns\n",
    "df_dropped = df.drop(['Bonus', 'Total_Comp'], axis=1)  # axis=1 for columns, axis=0 for rows\n",
    "print(\"DataFrame after dropping columns:\")\n",
    "print(df_dropped)\n",
    "print()\n",
    "\n",
    "# Sorting data\n",
    "print(\"DataFrame sorted by Age (ascending):\")\n",
    "print(df.sort_values('Age'))\n",
    "print(\"\\nDataFrame sorted by Salary (descending):\")\n",
    "print(df.sort_values('Salary', ascending=False))\n",
    "print(\"\\nDataFrame sorted by Department, then by Salary:\")\n",
    "print(df.sort_values(['Department', 'Salary'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Handling Missing Data\n",
    "\n",
    "Missing data is common in real-world datasets. Pandas provides tools for detecting, removing, and filling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "data_missing = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': [1, 2, 3, np.nan, np.nan],\n",
    "    'D': [1, 2, 3, 4, 5]\n",
    "}\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_missing)\n",
    "print()\n",
    "\n",
    "# Detecting missing values\n",
    "print(\"Missing value mask (True where values are missing):\")\n",
    "print(df_missing.isna())  # Returns a boolean DataFrame\n",
    "print(\"\\nCount of missing values per column:\")\n",
    "print(df_missing.isna().sum())\n",
    "print(\"\\nAny missing values in DataFrame?\", df_missing.isna().any().any())\n",
    "print()\n",
    "\n",
    "# Dropping missing values\n",
    "print(\"Drop rows with any missing values:\")\n",
    "print(df_missing.dropna())  # Returns a new DataFrame\n",
    "print(\"\\nDrop rows where all values are missing:\")\n",
    "print(df_missing.dropna(how='all'))  # No change in this example\n",
    "print(\"\\nDrop columns with any missing values:\")\n",
    "print(df_missing.dropna(axis=1))  # Only column D remains\n",
    "print()\n",
    "\n",
    "# Filling missing values\n",
    "print(\"Fill missing values with 0:\")\n",
    "print(df_missing.fillna(0))\n",
    "print(\"\\nFill missing values with column means:\")\n",
    "print(df_missing.fillna(df_missing.mean()))\n",
    "print(\"\\nFill missing values with different values for each column:\")\n",
    "print(df_missing.fillna({'A': 0, 'B': -1, 'C': 999}))\n",
    "print(\"\\nFill missing values using forward fill (propagate last valid observation):\")\n",
    "print(df_missing.fillna(method='ffill'))\n",
    "print(\"\\nFill missing values using backward fill (use next valid observation):\")\n",
    "print(df_missing.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Aggregation and Grouping\n",
    "\n",
    "### Concept: Grouping Data\n",
    "\n",
    "The `groupby` operation is one of the most powerful features in Pandas. It allows you to split data into groups based on some criteria, apply a function to each group independently, and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Department': ['IT', 'HR', 'IT', 'Finance', 'Marketing', 'HR', 'IT', 'Finance'],\n",
    "    'Employee': ['John', 'Anna', 'Peter', 'Linda', 'Bob', 'Sarah', 'Michael', 'Emma'],\n",
    "    'Salary': [65000, 70000, 62000, 85000, 72000, 69000, 75000, 82000],\n",
    "    'Experience': [3, 5, 2, 10, 7, 4, 8, 6],\n",
    "    'Gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Group by a single column\n",
    "dept_group = df.groupby('Department')\n",
    "\n",
    "# Aggregation: compute statistics for each group\n",
    "print(\"Mean salary by department:\")\n",
    "print(dept_group['Salary'].mean())\n",
    "print(\"\\nCount of employees by department:\")\n",
    "print(dept_group.size())\n",
    "print(\"\\nMultiple statistics by department:\")\n",
    "print(dept_group['Salary'].agg(['min', 'max', 'mean', 'std']))\n",
    "print()\n",
    "\n",
    "# Multiple aggregations on different columns\n",
    "print(\"Different aggregations for different columns:\")\n",
    "print(dept_group.agg({\n",
    "    'Salary': ['min', 'max', 'mean'],\n",
    "    'Experience': ['min', 'max', 'mean']\n",
    "}))\n",
    "print()\n",
    "\n",
    "# Group by multiple columns\n",
    "dept_gender_group = df.groupby(['Department', 'Gender'])\n",
    "print(\"Mean salary by department and gender:\")\n",
    "print(dept_gender_group['Salary'].mean())\n",
    "print(\"\\nCount of employees by department and gender:\")\n",
    "print(dept_gender_group.size().unstack())  # Reshape to a more readable format\n",
    "print()\n",
    "\n",
    "# Transformation: apply a function to each group and return a DataFrame with the same shape\n",
    "print(\"Salary relative to department average:\")\n",
    "df['Relative_Salary'] = df['Salary'] / dept_group['Salary'].transform('mean')\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Filter: select groups that satisfy a condition\n",
    "print(\"Departments with average salary > 70000:\")\n",
    "high_salary_depts = dept_group.filter(lambda x: x['Salary'].mean() > 70000)\n",
    "print(high_salary_depts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Pivot Tables and Cross-Tabulation\n",
    "\n",
    "Pivot tables and cross-tabulation are powerful tools for summarizing and reshaping data. They allow you to create spreadsheet-like pivot tables and contingency tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the same DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df[['Department', 'Employee', 'Salary', 'Experience', 'Gender']])  # Exclude Relative_Salary\n",
    "print()\n",
    "\n",
    "# Create a pivot table\n",
    "print(\"Pivot table: Mean salary by Department and Gender:\")\n",
    "pivot = pd.pivot_table(df, values='Salary', index='Department', columns='Gender', aggfunc='mean')\n",
    "print(pivot)\n",
    "print()\n",
    "\n",
    "# More complex pivot table with multiple values and aggregations\n",
    "print(\"Complex pivot table with multiple values and aggregations:\")\n",
    "complex_pivot = pd.pivot_table(\n",
    "    df, \n",
    "    values=['Salary', 'Experience'], \n",
    "    index='Department',\n",
    "    columns='Gender',\n",
    "    aggfunc={'Salary': ['mean', 'sum'], 'Experience': ['mean', 'min', 'max']}\n",
    ")\n",
    "print(complex_pivot)\n",
    "print()\n",
    "\n",
    "# Cross-tabulation (contingency table)\n",
    "# Add a categorical column for experience level\n",
    "df['Experience_Level'] = pd.cut(df['Experience'], bins=[0, 3, 7, 100], labels=['Junior', 'Mid', 'Senior'])\n",
    "print(\"DataFrame with Experience_Level:\")\n",
    "print(df[['Department', 'Employee', 'Experience', 'Experience_Level']])\n",
    "print()\n",
    "\n",
    "# Create a cross-tabulation\n",
    "print(\"Cross-tabulation of Department and Experience_Level:\")\n",
    "ct = pd.crosstab(df['Department'], df['Experience_Level'])\n",
    "print(ct)\n",
    "print()\n",
    "\n",
    "# Cross-tabulation with normalization\n",
    "print(\"Normalized cross-tabulation (row percentages):\")\n",
    "ct_norm = pd.crosstab(df['Department'], df['Experience_Level'], normalize='index')\n",
    "print(ct_norm)\n",
    "print()\n",
    "\n",
    "# Cross-tabulation with multiple indices\n",
    "print(\"Cross-tabulation with Department and Gender vs Experience_Level:\")\n",
    "ct_multi = pd.crosstab([df['Department'], df['Gender']], df['Experience_Level'])\n",
    "print(ct_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Merging and Joining DataFrames\n",
    "\n",
    "Pandas provides several methods for combining DataFrames, similar to SQL joins. These operations are essential for integrating data from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "# Employee data\n",
    "employees = pd.DataFrame({\n",
    "    'employee_id': [101, 102, 103, 104, 105],\n",
    "    'name': ['John', 'Anna', 'Peter', 'Linda', 'Bob'],\n",
    "    'department_id': [1, 2, 1, 3, 4]\n",
    "})\n",
    "\n",
    "# Department data\n",
    "departments = pd.DataFrame({\n",
    "    'department_id': [1, 2, 3, 5],\n",
    "    'department_name': ['IT', 'HR', 'Finance', 'Marketing']\n",
    "})\n",
    "\n",
    "# Salary data\n",
    "salaries = pd.DataFrame({\n",
    "    'employee_id': [101, 102, 103, 104, 106],\n",
    "    'salary': [65000, 70000, 62000, 85000, 72000]\n",
    "})\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments DataFrame:\")\n",
    "print(departments)\n",
    "print(\"\\nSalaries DataFrame:\")\n",
    "print(salaries)\n",
    "print()\n",
    "\n",
    "# Inner join (only matching keys in both DataFrames)\n",
    "print(\"Inner join of employees and departments:\")\n",
    "inner_join = pd.merge(employees, departments, on='department_id', how='inner')\n",
    "print(inner_join)  # Employee 105 (Bob) is excluded because department_id 4 is not in departments\n",
    "print()\n",
    "\n",
    "# Left join (all keys from left DataFrame)\n",
    "print(\"Left join of employees and departments:\")\n",
    "left_join = pd.merge(employees, departments, on='department_id', how='left')\n",
    "print(left_join)  # Employee 105 (Bob) is included with NaN for department_name\n",
    "print()\n",
    "\n",
    "# Right join (all keys from right DataFrame)\n",
    "print(\"Right join of employees and departments:\")\n",
    "right_join = pd.merge(employees, departments, on='department_id', how='right')\n",
    "print(right_join)  # Department 5 (Marketing) is included with NaN for employee data\n",
    "print()\n",
    "\n",
    "# Outer join (all keys from both DataFrames)\n",
    "print(\"Outer join of employees and departments:\")\n",
    "outer_join = pd.merge(employees, departments, on='department_id', how='outer')\n",
    "print(outer_join)  # Both Employee 105 and Department 5 are included with NaNs\n",
    "print()\n",
    "\n",
    "# Multiple joins\n",
    "print(\"Joining employees, departments, and salaries:\")\n",
    "# First join employees and departments\n",
    "emp_dept = pd.merge(employees, departments, on='department_id', how='left')\n",
    "# Then join with salaries\n",
    "emp_dept_sal = pd.merge(emp_dept, salaries, on='employee_id', how='left')\n",
    "print(emp_dept_sal)\n",
    "print()\n",
    "\n",
    "# Joining on different column names\n",
    "# Rename department_id in departments to dept_id\n",
    "departments_renamed = departments.rename(columns={'department_id': 'dept_id'})\n",
    "print(\"Joining with different column names:\")\n",
    "print(pd.merge(employees, departments_renamed, left_on='department_id', right_on='dept_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-world Applications of Pandas in Machine Learning\n",
    "\n",
    "### 4.1 Data Loading and Preprocessing\n",
    "\n",
    "Pandas is essential for loading and preprocessing data before feeding it into machine learning models. Let's explore a real-world example using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic = fetch_openml(name='titanic', version=1, as_frame=True)\n",
    "df_titanic = titanic.data\n",
    "df_titanic['survived'] = titanic.target\n",
    "\n",
    "print(\"Titanic dataset shape:\", df_titanic.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_titanic.head())\n",
    "print(\"\\nDataset information:\")\n",
    "df_titanic.info()\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df_titanic.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_titanic.isna().sum())\n",
    "\n",
    "# Data preprocessing\n",
    "# 1. Select relevant features\n",
    "df_selected = df_titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'survived']]\n",
    "\n",
    "# 2. Handle missing values\n",
    "# Fill missing age with median\n",
    "df_selected['age'] = df_selected['age'].fillna(df_selected['age'].median())\n",
    "# Fill missing embarked with most common value\n",
    "df_selected['embarked'] = df_selected['embarked'].fillna(df_selected['embarked'].mode()[0])\n",
    "\n",
    "# 3. Convert categorical variables to numeric\n",
    "# Convert sex to numeric (0 for female, 1 for male)\n",
    "df_selected['sex'] = df_selected['sex'].map({'female': 0, 'male': 1})\n",
    "# Convert embarked to numeric using one-hot encoding\n",
    "embarked_dummies = pd.get_dummies(df_selected['embarked'], prefix='embarked')\n",
    "df_selected = pd.concat([df_selected, embarked_dummies], axis=1)\n",
    "df_selected.drop('embarked', axis=1, inplace=True)\n",
    "\n",
    "print(\"\\nPreprocessed dataset:\")\n",
    "print(df_selected.head())\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(df_selected.isna().sum())\n",
    "\n",
    "# Visualize survival rate by passenger class\n",
    "survival_by_class = df_selected.groupby('pclass')['survived'].mean()\n",
    "plt.figure(figsize=(10, 6))\n",
    "survival_by_class.plot(kind='bar', color='skyblue')\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Visualize survival rate by sex\n",
    "survival_by_sex = df_selected.groupby('sex')['survived'].mean()\n",
    "plt.figure(figsize=(10, 6))\n",
    "survival_by_sex.plot(kind='bar', color='salmon')\n",
    "plt.title('Survival Rate by Sex')\n",
    "plt.xlabel('Sex (0=Female, 1=Male)')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Data Preprocessing Pipeline\n",
    "\n",
    "In the Titanic example above, we followed a typical data preprocessing pipeline for machine learning:\n",
    "\n",
    "1. **Data Loading**: Using Pandas to load and inspect the dataset\n",
    "2. **Feature Selection**: Selecting relevant columns for our analysis\n",
    "3. **Missing Value Handling**: Filling missing values with appropriate strategies (median for age, mode for embarked)\n",
    "4. **Categorical Encoding**: Converting categorical variables to numeric (mapping for binary, one-hot encoding for multi-category)\n",
    "5. **Data Validation**: Checking for any remaining issues before proceeding to modeling\n",
    "\n",
    "This preprocessing pipeline is essential for preparing data for machine learning algorithms, which typically require numeric inputs without missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exploratory Data Analysis (EDA) with Pandas\n",
    "\n",
    "Exploratory Data Analysis is a critical step in any data science project. Pandas provides powerful tools for exploring and visualizing data to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the Titanic dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for visualizations\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Age distribution by survival status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df_selected, x='age', hue='survived', bins=30, kde=True, element='step')\n",
    "plt.title('Age Distribution by Survival Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Survived', labels=['No', 'Yes'])\n",
    "plt.show()\n",
    "\n",
    "# Fare distribution by passenger class\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_selected, x='pclass', y='fare')\n",
    "plt.title('Fare Distribution by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_selected.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Survival rate by passenger class and sex\n",
    "survival_by_class_sex = df_selected.groupby(['pclass', 'sex'])['survived'].mean().unstack()\n",
    "plt.figure(figsize=(12, 6))\n",
    "survival_by_class_sex.plot(kind='bar', colormap='Set2')\n",
    "plt.title('Survival Rate by Passenger Class and Sex')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(title='Sex', labels=['Female', 'Male'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Age distribution by passenger class\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=df_selected, x='pclass', y='age')\n",
    "plt.title('Age Distribution by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Age')\n",
    "plt.show()\n",
    "\n",
    "# Survival rate by family size (sibsp + parch)\n",
    "df_selected['family_size'] = df_selected['sibsp'] + df_selected['parch']\n",
    "survival_by_family = df_selected.groupby('family_size')['survived'].mean()\n",
    "plt.figure(figsize=(12, 6))\n",
    "survival_by_family.plot(kind='bar', color='teal')\n",
    "plt.title('Survival Rate by Family Size')\n",
    "plt.xlabel('Family Size')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Exploratory Data Analysis Techniques\n",
    "\n",
    "In the EDA example above, we used several techniques to explore the Titanic dataset:\n",
    "\n",
    "1. **Univariate Analysis**: Examining the distribution of individual variables (age, fare)\n",
    "2. **Bivariate Analysis**: Exploring relationships between pairs of variables (age vs. survival, fare vs. class)\n",
    "3. **Multivariate Analysis**: Investigating interactions between multiple variables (survival by class and sex)\n",
    "4. **Correlation Analysis**: Measuring the strength and direction of relationships between variables\n",
    "5. **Feature Engineering**: Creating new features (family_size) to gain additional insights\n",
    "\n",
    "These EDA techniques help us understand the data better, identify patterns and relationships, and inform our feature engineering and modeling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Time Series Analysis with Pandas\n",
    "\n",
    "Pandas provides powerful tools for working with time series data, which is common in many machine learning applications such as forecasting, anomaly detection, and trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a date range\n",
    "date_rng = pd.date_range(start='2020-01-01', end='2022-12-31', freq='D')\n",
    "print(f\"Date range length: {len(date_rng)} days\")\n",
    "print(f\"First few dates: {date_rng[:5]}\")\n",
    "print(f\"Last few dates: {date_rng[-5:]}\")\n",
    "print()\n",
    "\n",
    "# Create a time series with a trend, seasonality, and noise\n",
    "np.random.seed(42)\n",
    "df_ts = pd.DataFrame(date_rng, columns=['date'])\n",
    "df_ts['day_of_week'] = df_ts['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_ts['month'] = df_ts['date'].dt.month\n",
    "df_ts['year'] = df_ts['date'].dt.year\n",
    "\n",
    "# Create components\n",
    "n = len(df_ts)\n",
    "# Trend: linear increase over time\n",
    "trend = np.linspace(100, 200, n)\n",
    "# Seasonality: yearly cycle\n",
    "yearly_seasonality = 20 * np.sin(2 * np.pi * np.arange(n) / 365)\n",
    "# Weekly seasonality: higher on weekends\n",
    "weekly_seasonality = 15 * (df_ts['day_of_week'] >= 5).astype(int)\n",
    "# Random noise\n",
    "noise = 10 * np.random.randn(n)\n",
    "\n",
    "# Combine components\n",
    "df_ts['value'] = trend + yearly_seasonality + weekly_seasonality + noise\n",
    "\n",
    "# Set date as index\n",
    "df_ts.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Time series dataset:\")\n",
    "print(df_ts.head())\n",
    "print()\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_ts.index, df_ts['value'])\n",
    "plt.title('Time Series Data (2020-2022)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Resampling: Aggregate to monthly frequency\n",
    "monthly_mean = df_ts['value'].resample('M').mean()\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(monthly_mean.index, monthly_mean, marker='o')\n",
    "plt.title('Monthly Average Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Rolling statistics\n",
    "rolling_mean = df_ts['value'].rolling(window=30).mean()\n",
    "rolling_std = df_ts['value'].rolling(window=30).std()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_ts.index, df_ts['value'], alpha=0.5, label='Original')\n",
    "plt.plot(rolling_mean.index, rolling_mean, label='30-day Rolling Mean')\n",
    "plt.fill_between(rolling_std.index, \n",
    "                 rolling_mean - 2*rolling_std, \n",
    "                 rolling_mean + 2*rolling_std, \n",
    "                 color='gray', alpha=0.2, label='±2 Std Dev')\n",
    "plt.title('Time Series with 30-day Rolling Mean and Standard Deviation')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Resample to weekly to make the decomposition clearer\n",
    "weekly_mean = df_ts['value'].resample('W').mean()\n",
    "result = seasonal_decompose(weekly_mean, model='additive', period=52)  # 52 weeks in a year\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 12))\n",
    "result.observed.plot(ax=ax1)\n",
    "ax1.set_title('Observed')\n",
    "ax1.grid(True)\n",
    "result.trend.plot(ax=ax2)\n",
    "ax2.set_title('Trend')\n",
    "ax2.grid(True)\n",
    "result.seasonal.plot(ax=ax3)\n",
    "ax3.set_title('Seasonal')\n",
    "ax3.grid(True)\n",
    "result.resid.plot(ax=ax4)\n",
    "ax4.set_title('Residual')\n",
    "ax4.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shift operations for lagged features\n",
    "df_ts['lag_1'] = df_ts['value'].shift(1)  # Previous day\n",
    "df_ts['lag_7'] = df_ts['value'].shift(7)  # Previous week\n",
    "df_ts['lag_30'] = df_ts['value'].shift(30)  # Previous month\n",
    "\n",
    "print(\"Time series with lagged features:\")\n",
    "print(df_ts.head(10))\n",
    "print()\n",
    "\n",
    "# Calculate correlation with lagged values\n",
    "print(\"Correlation with lagged values:\")\n",
    "print(df_ts[['value', 'lag_1', 'lag_7', 'lag_30']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Time Series Analysis Techniques\n",
    "\n",
    "In the time series example above, we demonstrated several key techniques for working with time series data in Pandas:\n",
    "\n",
    "1. **Date Indexing**: Using DatetimeIndex to efficiently store and access time-based data\n",
    "2. **Date Attributes**: Extracting components like day of week, month, and year\n",
    "3. **Resampling**: Changing the frequency of time series data (e.g., daily to monthly)\n",
    "4. **Rolling Statistics**: Computing moving averages and other statistics over a sliding window\n",
    "5. **Seasonal Decomposition**: Breaking down a time series into trend, seasonal, and residual components\n",
    "6. **Lagged Features**: Creating shifted versions of the data for use in predictive models\n",
    "\n",
    "These techniques are essential for time series forecasting, anomaly detection, and other time-based machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building a Machine Learning Pipeline with Pandas\n",
    "\n",
    "Pandas integrates seamlessly with scikit-learn to build end-to-end machine learning pipelines. Let's demonstrate this with a classification task using the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a machine learning pipeline with Pandas and scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Titanic dataset again\n",
    "from sklearn.datasets import fetch_openml\n",
    "titanic = fetch_openml(name='titanic', version=1, as_frame=True)\n",
    "X = titanic.data\n",
    "y = titanic.target.astype(int)  # Convert to integer\n",
    "\n",
    "# Select features for the model\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "X = X[features]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print()\n",
    "\n",
    "# Define preprocessing for numeric columns (impute missing values and scale)\n",
    "numeric_features = ['age', 'sibsp', 'parch', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical columns (impute missing values and one-hot encode)\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the full pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Did not survive', 'Survived'],\n",
    "            yticklabels=['Did not survive', 'Survived'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "print()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "print()\n",
    "\n",
    "# Feature importance\n",
    "# Get feature names after preprocessing\n",
    "preprocessor.fit(X)\n",
    "feature_names = (\n",
    "    numeric_features +\n",
    "    list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Train the best model\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "best_rf = best_pipeline.named_steps['classifier']\n",
    "feature_importances = best_rf.feature_importances_\n",
    "\n",
    "# Sort feature importances\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_feature_names = [feature_names[i] for i in indices]\n",
    "sorted_importances = feature_importances[indices]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(sorted_importances)), sorted_importances, align='center')\n",
    "plt.yticks(range(len(sorted_importances)), sorted_feature_names)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Machine Learning Pipeline Components\n",
    "\n",
    "In the machine learning pipeline example above, we demonstrated how Pandas integrates with scikit-learn to build a complete workflow:\n",
    "\n",
    "1. **Data Preparation**: Using Pandas to load and organize the data\n",
    "2. **Feature Selection**: Choosing relevant columns for the model\n",
    "3. **Data Splitting**: Dividing the data into training and test sets\n",
    "4. **Preprocessing Pipeline**: Creating transformers for numeric and categorical features\n",
    "5. **Model Training**: Fitting the model on the preprocessed training data\n",
    "6. **Evaluation**: Assessing model performance on the test set\n",
    "7. **Cross-Validation**: Validating the model on multiple data splits\n",
    "8. **Hyperparameter Tuning**: Finding the optimal model configuration\n",
    "9. **Feature Importance Analysis**: Understanding which features contribute most to predictions\n",
    "\n",
    "This end-to-end pipeline demonstrates how Pandas serves as the foundation for data handling in machine learning workflows, seamlessly integrating with scikit-learn's preprocessing and modeling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "Now that you've learned the fundamentals of Pandas, try solving these practice problems to test your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Data Cleaning and Transformation\n",
    "\n",
    "Create a DataFrame with the following data:\n",
    "```\n",
    "data = {\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Mary Williams', np.nan],\n",
    "    'Age': [28, 34, np.nan, 42, 45],\n",
    "    'City': ['New York', None, 'Chicago', 'Boston', 'Seattle'],\n",
    "    'Salary': ['$65,000', '$70,000', '$62,000', '$85,000', '$72,000']\n",
    "}\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Fill missing names with 'Unknown'\n",
    "2. Fill missing ages with the median age\n",
    "3. Fill missing cities with 'Other'\n",
    "4. Convert salary strings to integers (remove '$' and ',')\n",
    "5. Add a new column 'Salary_Category' with values 'Low' (< 65000), 'Medium' (65000-75000), or 'High' (> 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the DataFrame\n",
    "data = {\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Mary Williams', np.nan],\n",
    "    'Age': [28, 34, np.nan, 42, 45],\n",
    "    'City': ['New York', None, 'Chicago', 'Boston', 'Seattle'],\n",
    "    'Salary': ['$65,000', '$70,000', '$62,000', '$85,000', '$72,000']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# 1. Fill missing names with 'Unknown'\n",
    "df['Name'] = df['Name'].fillna('Unknown')\n",
    "\n",
    "# 2. Fill missing ages with the median age\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "\n",
    "# 3. Fill missing cities with 'Other'\n",
    "df['City'] = df['City'].fillna('Other')\n",
    "\n",
    "# 4. Convert salary strings to integers\n",
    "df['Salary'] = df['Salary'].str.replace('$', '').str.replace(',', '').astype(int)\n",
    "\n",
    "# 5. Add a new column 'Salary_Category'\n",
    "def categorize_salary(salary):\n",
    "    if salary < 65000:\n",
    "        return 'Low'\n",
    "    elif salary <= 75000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['Salary_Category'] = df['Salary'].apply(categorize_salary)\n",
    "\n",
    "print(\"Cleaned and transformed DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Data Aggregation and Grouping\n",
    "\n",
    "Create a DataFrame with the following data:\n",
    "```\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "    'Store': np.random.choice(['A', 'B', 'C', 'D'], size=100),\n",
    "    'Product': np.random.choice(['Widget', 'Gadget', 'Tool', 'Device'], size=100),\n",
    "    'Sales': np.random.randint(10, 100, size=100),\n",
    "    'Revenue': np.random.randint(100, 1000, size=100)\n",
    "}\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Group by Store and Product, and calculate the total Sales and average Revenue\n",
    "2. Find the Store with the highest total Revenue\n",
    "3. Calculate monthly Sales and Revenue for each Store\n",
    "4. Create a pivot table showing total Revenue by Store (rows) and Product (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the DataFrame\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "    'Store': np.random.choice(['A', 'B', 'C', 'D'], size=100),\n",
    "    'Product': np.random.choice(['Widget', 'Gadget', 'Tool', 'Device'], size=100),\n",
    "    'Sales': np.random.randint(10, 100, size=100),\n",
    "    'Revenue': np.random.randint(100, 1000, size=100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame (first 5 rows):\")\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "# 1. Group by Store and Product, calculate total Sales and average Revenue\n",
    "store_product_stats = df.groupby(['Store', 'Product']).agg({\n",
    "    'Sales': 'sum',\n",
    "    'Revenue': 'mean'\n",
    "}).rename(columns={'Revenue': 'Avg_Revenue'})\n",
    "print(\"Total Sales and Average Revenue by Store and Product:\")\n",
    "print(store_product_stats)\n",
    "print()\n",
    "\n",
    "# 2. Find the Store with the highest total Revenue\n",
    "store_revenue = df.groupby('Store')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(\"Stores ranked by total Revenue:\")\n",
    "print(store_revenue)\n",
    "highest_revenue_store = store_revenue.index[0]\n",
    "print(f\"\\nStore with highest total Revenue: {highest_revenue_store} (${store_revenue.iloc[0]}\")\n",
    "print()\n",
    "\n",
    "# 3. Calculate monthly Sales and Revenue for each Store\n",
    "# Add month column\n",
    "df['Month'] = df['Date'].dt.to_period('M')\n",
    "monthly_store_stats = df.groupby(['Month', 'Store']).agg({\n",
    "    'Sales': 'sum',\n",
    "    'Revenue': 'sum'\n",
    "})\n",
    "print(\"Monthly Sales and Revenue by Store:\")\n",
    "print(monthly_store_stats)\n",
    "print()\n",
    "\n",
    "# 4. Create a pivot table showing total Revenue by Store and Product\n",
    "revenue_pivot = pd.pivot_table(df, values='Revenue', index='Store', columns='Product', aggfunc='sum')\n",
    "print(\"Pivot Table: Total Revenue by Store and Product:\")\n",
    "print(revenue_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Data Merging and Transformation\n",
    "\n",
    "Create three DataFrames:\n",
    "\n",
    "```python\n",
    "# Customer data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'city': ['New York', 'Chicago', 'New York', 'Boston', 'Chicago']\n",
    "})\n",
    "\n",
    "# Order data\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'customer_id': [1, 2, 3, 3, 4, 6, 5],\n",
    "    'order_date': ['2023-01-15', '2023-01-20', '2023-02-05', '2023-02-10', '2023-02-15', '2023-03-01', '2023-03-05'],\n",
    "    'amount': [150, 200, 300, 150, 250, 100, 400]\n",
    "})\n",
    "\n",
    "# Product data\n",
    "products = pd.DataFrame({\n",
    "    'order_id': [101, 101, 102, 103, 104, 105, 106, 107],\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'A', 'D', 'C'],\n",
    "    'quantity': [2, 1, 3, 2, 1, 4, 2, 3]\n",
    "})\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Merge the three DataFrames to create a complete view of customers, orders, and products\n",
    "2. Calculate the total amount spent by each customer\n",
    "3. Find the most popular product (highest total quantity)\n",
    "4. Calculate the average order amount by city\n",
    "5. Identify customers who have ordered product 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import pandas as pd\n",
    "\n",
    "# Create the DataFrames\n",
    "# Customer data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'city': ['New York', 'Chicago', 'New York', 'Boston', 'Chicago']\n",
    "})\n",
    "\n",
    "# Order data\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'customer_id': [1, 2, 3, 3, 4, 6, 5],\n",
    "    'order_date': ['2023-01-15', '2023-01-20', '2023-02-05', '2023-02-10', '2023-02-15', '2023-03-01', '2023-03-05'],\n",
    "    'amount': [150, 200, 300, 150, 250, 100, 400]\n",
    "})\n",
    "\n",
    "# Product data\n",
    "products = pd.DataFrame({\n",
    "    'order_id': [101, 101, 102, 103, 104, 105, 106, 107],\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'A', 'D', 'C'],\n",
    "    'quantity': [2, 1, 3, 2, 1, 4, 2, 3]\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)\n",
    "print(\"\\nProducts:\")\n",
    "print(products)\n",
    "print()\n",
    "\n",
    "# 1. Merge the three DataFrames\n",
    "# First, merge orders and customers\n",
    "orders_customers = pd.merge(orders, customers, on='customer_id', how='left')\n",
    "# Then, merge with products\n",
    "complete_data = pd.merge(orders_customers, products, on='order_id', how='left')\n",
    "print(\"Complete merged data:\")\n",
    "print(complete_data.head())\n",
    "print()\n",
    "\n",
    "# 2. Calculate the total amount spent by each customer\n",
    "customer_spending = orders.groupby('customer_id')['amount'].sum().reset_index()\n",
    "customer_spending = pd.merge(customer_spending, customers[['customer_id', 'name']], on='customer_id', how='left')\n",
    "print(\"Total amount spent by each customer:\")\n",
    "print(customer_spending.sort_values('amount', ascending=False))\n",
    "print()\n",
    "\n",
    "# 3. Find the most popular product (highest total quantity)\n",
    "product_popularity = products.groupby('product')['quantity'].sum().sort_values(ascending=False)\n",
    "print(\"Products by popularity (total quantity):\")\n",
    "print(product_popularity)\n",
    "most_popular = product_popularity.index[0]\n",
    "print(f\"\\nMost popular product: {most_popular} (total quantity: {product_popularity.iloc[0]})\")\n",
    "print()\n",
    "\n",
    "# 4. Calculate the average order amount by city\n",
    "city_avg_order = orders_customers.groupby('city')['amount'].mean()\n",
    "print(\"Average order amount by city:\")\n",
    "print(city_avg_order)\n",
    "print()\n",
    "\n",
    "# 5. Identify customers who have ordered product 'A'\n",
    "customers_product_a = complete_data[complete_data['product'] == 'A'][['customer_id', 'name']].drop_duplicates()\n",
    "print(\"Customers who ordered product 'A':\")\n",
    "print(customers_product_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "To further enhance your Pandas skills, check out these resources:\n",
    "\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Hands-On Data Analysis with Pandas](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition)\n",
    "- [Data Analysis Projects With Pandas](https://github.com/mohammadreza-mohammadi94/Data-Analysis-Projects-With-Pandas)\n",
    "- [Python for Data Analysis](https://wesmckinney.com/book/) by Wes McKinney (creator of pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
